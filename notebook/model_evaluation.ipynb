{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bdd922c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03fc9cd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\Production\\\\projects\\\\brain_tumor_classification\\\\notebook'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e267ae97",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c15c182b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\Production\\\\projects\\\\brain_tumor_classification'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a990f5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import mlflow\n",
    "\n",
    "# DAGsHub credentials\n",
    "os.environ[\"MLFLOW_TRACKING_URI\"] = \"https://dagshub.com/hafizshakeel/brain_tumor_classification.mlflow\"\n",
    "os.environ[\"MLFLOW_TRACKING_USERNAME\"] = \"hafizshakeel\"  # settings -> profile -> username \n",
    "os.environ[\"MLFLOW_TRACKING_PASSWORD\"] = \"=**********\"  #  # settings -> tokens \n",
    "\n",
    "# https://dagshub.com/user/settings/tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fe9935d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class EvaluationConfig:\n",
    "    root_dir: Path\n",
    "    trained_model_path: Path\n",
    "    test_data_dir: Path\n",
    "    report: Path\n",
    "    batch_size: int\n",
    "    log_with_mlflow: bool\n",
    "    mlflow_experiment: str\n",
    "    mlflow_tracking_uri: str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3823e40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import mlflow\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from src.brain_tumor_classification import logger\n",
    "from src.brain_tumor_classification.constants import *\n",
    "from src.brain_tumor_classification.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "467e3a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(self,\n",
    "                 config_filepath=CONFIG_FILE_PATH,\n",
    "                 params_filepath=PARAMS_FILE_PATH):\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def get_evaluation_config(self) -> EvaluationConfig:\n",
    "        config = self.config.evaluation\n",
    "        params = self.params.evaluation\n",
    "        mlflow_config = self.config.mlflow\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        evaluation_config = EvaluationConfig(\n",
    "            root_dir=Path(config.root_dir),\n",
    "            trained_model_path=Path(self.config.training.trained_model_path),\n",
    "            test_data_dir=Path(config.test_data_dir),\n",
    "            report=Path(config.report),\n",
    "            batch_size=params.batch_size,\n",
    "            log_with_mlflow=mlflow_config.log_with_mlflow,\n",
    "            mlflow_experiment=mlflow_config.mlflow_experiment,\n",
    "            mlflow_tracking_uri=mlflow_config.mlflow_tracking_uri\n",
    "        )\n",
    "        return evaluation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b63cd424",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvaluationPipeline:\n",
    "    def __init__(self, config, device):\n",
    "        self.config = config\n",
    "        self.device = device\n",
    "\n",
    "        # Data transforms\n",
    "        self.test_transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5], [0.5])\n",
    "        ])\n",
    "\n",
    "        # Dataset & dataloader\n",
    "        self.test_dataset = datasets.ImageFolder(self.config.test_data_dir, transform=self.test_transform)\n",
    "        self.test_loader = DataLoader(self.test_dataset, batch_size=self.config.batch_size, shuffle=False)\n",
    "\n",
    "        self.class_names = self.test_dataset.classes\n",
    "\n",
    "        # Load model\n",
    "        self.model = self._load_model()\n",
    "\n",
    "    def _load_model(self):\n",
    "        \"\"\"Safely load the model with proper error handling\"\"\"\n",
    "        try:\n",
    "            # Try to load the entire model first\n",
    "            model = torch.load(self.config.trained_model_path, map_location=self.device, weights_only=False)\n",
    "            logger.info(\"Loaded entire model successfully\")\n",
    "            return model\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Could not load entire model: {e}. Trying to load state dict...\")\n",
    "            try:\n",
    "                # If that fails, try to load just the state dict\n",
    "                # This assumes you know the model architecture\n",
    "                from torchvision.models import swin_t\n",
    "                model = swin_t(weights=None)\n",
    "                num_ftrs = model.head.in_features\n",
    "                model.head = torch.nn.Linear(num_ftrs, len(self.class_names))\n",
    "                \n",
    "                state_dict = torch.load(self.config.trained_model_path, map_location=self.device)\n",
    "                model.load_state_dict(state_dict)\n",
    "                logger.info(\"Loaded model state dict successfully\")\n",
    "                return model\n",
    "            except Exception as e2:\n",
    "                logger.error(f\"Could not load model state dict either: {e2}\")\n",
    "                raise\n",
    "\n",
    "    def run(self):\n",
    "        logger.info(\"Starting Evaluation...\")\n",
    "        \n",
    "        # Set model to evaluation mode\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "        all_labels, all_preds = [], []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in self.test_loader:\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                outputs = self.model(images)\n",
    "                _, predicted = outputs.max(1)\n",
    "\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "                all_preds.extend(predicted.cpu().numpy())\n",
    "\n",
    "        # Compute metrics\n",
    "        report_dict = classification_report(\n",
    "            all_labels,\n",
    "            all_preds,\n",
    "            target_names=self.class_names,\n",
    "            digits=4,\n",
    "            output_dict=True,\n",
    "            zero_division=0\n",
    "        )\n",
    "        cm = confusion_matrix(all_labels, all_preds)\n",
    "        \n",
    "        logger.info(f\"\\nClassification Report:\\n{json.dumps(report_dict, indent=2)}\")\n",
    "        logger.info(f\"Confusion Matrix:\\n{cm}\")\n",
    "\n",
    "        # Save classification report\n",
    "        os.makedirs(os.path.dirname(self.config.report), exist_ok=True)\n",
    "        with open(self.config.report, \"w\") as f:\n",
    "            json.dump(report_dict, f, indent=4)\n",
    "\n",
    "        # MLflow logging\n",
    "        if self.config.log_with_mlflow:\n",
    "            try:\n",
    "                mlflow.set_tracking_uri(self.config.mlflow_tracking_uri)\n",
    "                mlflow.set_experiment(self.config.mlflow_experiment)\n",
    "                \n",
    "                with mlflow.start_run(run_name=\"evaluation_run\") as run:\n",
    "                    # Log parameters\n",
    "                    mlflow.log_params({\n",
    "                        \"batch_size\": self.config.batch_size,\n",
    "                        \"dataset\": \"test\",\n",
    "                        \"model_path\": str(self.config.trained_model_path)\n",
    "                    })\n",
    "                    \n",
    "                    # Log overall metrics\n",
    "                    mlflow.log_metrics({\n",
    "                        \"accuracy\": report_dict[\"accuracy\"],\n",
    "                        \"macro_precision\": report_dict[\"macro avg\"][\"precision\"],\n",
    "                        \"macro_recall\": report_dict[\"macro avg\"][\"recall\"],\n",
    "                        \"macro_f1\": report_dict[\"macro avg\"][\"f1-score\"],\n",
    "                        \"weighted_precision\": report_dict[\"weighted avg\"][\"precision\"],\n",
    "                        \"weighted_recall\": report_dict[\"weighted avg\"][\"recall\"],\n",
    "                        \"weighted_f1\": report_dict[\"weighted avg\"][\"f1-score\"],\n",
    "                    })\n",
    "                    \n",
    "                    # Log per-class metrics\n",
    "                    for class_name in self.class_names:\n",
    "                        if class_name in report_dict:\n",
    "                            mlflow.log_metrics({\n",
    "                                f\"precision_{class_name}\": report_dict[class_name][\"precision\"],\n",
    "                                f\"recall_{class_name}\": report_dict[class_name][\"recall\"],\n",
    "                                f\"f1_{class_name}\": report_dict[class_name][\"f1-score\"],\n",
    "                            }, step=0)\n",
    "                    \n",
    "                    # Log artifacts\n",
    "                    mlflow.log_artifact(self.config.report, artifact_path=\"reports\")\n",
    "                    \n",
    "                    # Log model\n",
    "                    mlflow.pytorch.log_model(self.model, \"evaluated_model\")\n",
    "\n",
    "                    logger.info(f\"MLflow run ID: {run.info.run_id}\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                logger.error(f\"MLflow logging failed: {e}\")\n",
    "                # Continue even if MLflow fails\n",
    "\n",
    "        logger.info(\"Evaluation Completed.\")\n",
    "        return report_dict, cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cddc9a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-02 12:35:13,238: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2025-09-02 12:35:13,245: INFO: common: yaml file: config\\params.yaml loaded successfully]\n",
      "[2025-09-02 12:35:13,248: INFO: common: created directory at: artifacts]\n",
      "[2025-09-02 12:35:13,251: INFO: common: created directory at: artifacts/evaluation]\n",
      "[2025-09-02 12:35:13,661: INFO: 1557871507: Loaded entire model successfully]\n",
      "[2025-09-02 12:35:13,669: INFO: 1557871507: Starting Evaluation...]\n",
      "[2025-09-02 12:35:17,531: INFO: 1557871507: \n",
      "Classification Report:\n",
      "{\n",
      "  \"glioma_tumor\": {\n",
      "    \"precision\": 0.2777777777777778,\n",
      "    \"recall\": 0.15,\n",
      "    \"f1-score\": 0.19480519480519481,\n",
      "    \"support\": 100.0\n",
      "  },\n",
      "  \"meningioma_tumor\": {\n",
      "    \"precision\": 0.4056603773584906,\n",
      "    \"recall\": 0.3739130434782609,\n",
      "    \"f1-score\": 0.3891402714932127,\n",
      "    \"support\": 115.0\n",
      "  },\n",
      "  \"no_tumor\": {\n",
      "    \"precision\": 0.8,\n",
      "    \"recall\": 0.0761904761904762,\n",
      "    \"f1-score\": 0.1391304347826087,\n",
      "    \"support\": 105.0\n",
      "  },\n",
      "  \"pituitary_tumor\": {\n",
      "    \"precision\": 0.26339285714285715,\n",
      "    \"recall\": 0.7972972972972973,\n",
      "    \"f1-score\": 0.3959731543624161,\n",
      "    \"support\": 74.0\n",
      "  },\n",
      "  \"accuracy\": 0.31725888324873097,\n",
      "  \"macro avg\": {\n",
      "    \"precision\": 0.4367077530697814,\n",
      "    \"recall\": 0.34935020424150853,\n",
      "    \"f1-score\": 0.27976226386085806,\n",
      "    \"support\": 394.0\n",
      "  },\n",
      "  \"weighted avg\": {\n",
      "    \"precision\": 0.4515730776715117,\n",
      "    \"recall\": 0.31725888324873097,\n",
      "    \"f1-score\": 0.2744729943584559,\n",
      "    \"support\": 394.0\n",
      "  }\n",
      "}]\n",
      "[2025-09-02 12:35:17,533: INFO: 1557871507: Confusion Matrix:\n",
      "[[15 29  1 55]\n",
      " [14 43  1 57]\n",
      " [13 31  8 53]\n",
      " [12  3  0 59]]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/02 12:35:27 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run evaluation_run at: https://dagshub.com/hafizshakeel/brain_tumor_classification.mlflow/#/experiments/3/runs/06cc170f18e341b299eee0fbc05b1ca3\n",
      "🧪 View experiment at: https://dagshub.com/hafizshakeel/brain_tumor_classification.mlflow/#/experiments/3\n",
      "[2025-09-02 12:35:31,385: ERROR: 1557871507: MLflow logging failed: INTERNAL_ERROR: Response: {'error': 'unsupported endpoint, please contact support@dagshub.com'}]\n",
      "[2025-09-02 12:35:31,387: INFO: 1557871507: Evaluation Completed.]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config_manager = ConfigurationManager()\n",
    "    eval_config = config_manager.get_evaluation_config()\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    evaluator = EvaluationPipeline(config=eval_config, device=device)\n",
    "    report, cm = evaluator.run()\n",
    "\n",
    "except Exception as e:\n",
    "    logger.exception(f\"Evaluation failed: {e}\")\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5d4d45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
